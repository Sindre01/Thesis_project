{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sys.path.append('../../')  # Add the path to the my_packages module\n",
    "os.environ['EXPERIMENT_DB_NAME'] = \"few_shot_experiments\"\n",
    "from my_packages.analysis.error_analysis import categorize_semantic_errors, categorize_syntax_error, categorize_syntax_parsing_error, categorize_test_errors, extract_semantic_errors, extract_test_error, get_error_category_counts, make_categories_bar_chart, make_categories_pie_chart\n",
    "from my_packages.db_service.error_service import delete_error_docs, errors_to_df, pretty_print_errors\n",
    "from my_packages.evaluation.midio_compiler import compile_code, is_code_syntax_valid\n",
    "from my_packages.db_service.experiment_service import experiment_exists, pretty_print_experiment_collections, run_experiment_quality_checks, setup_experiment_collection\n",
    "\n",
    "experiment_name = f\"signature_similarity_5_shot\"\n",
    "error_type = \"tests\"\n",
    "filter = {\n",
    "    \"eval_method\": \"3_fold\",\n",
    "    \"model_name\": \"llama3.2:3b-instruct-fp16\",\n",
    "    \"phase\": \"testing\",\n",
    "}\n",
    "# Categorize and extract errors:\n",
    "df = errors_to_df(experiment_name, filter=filter)\n",
    "print(df.count())\n",
    "df[\"syntax_error\"] = df[\"stderr\"]\n",
    "df[\"syntax_parsing_error\"] = df[\"stderr\"].apply(categorize_syntax_parsing_error)\n",
    "df[\"syntax_category\"] = df[\"stderr\"].apply(categorize_syntax_error)\n",
    "\n",
    "df[\"semantic_error\"] = df[\"error_msg\"].apply(extract_semantic_errors)\n",
    "df[\"semantic_category\"] = df[\"error_msg\"].apply(categorize_semantic_errors)\n",
    "\n",
    "df[\"tests_category\"] = df[\"test_result\"].apply(categorize_test_errors)\n",
    "df[\"tests_error\"] = df.apply(\n",
    "    lambda row: extract_test_error(row[\"tests_category\"], row[\"error_msg\"], row[\"test_result\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "filtered_df = df[\n",
    "    (df[\"error_type\"] == error_type) \n",
    "    # & (df[f\"{error_type}_category\"] == {'Symbol Resolution Error'})\n",
    "    # & (df[f\"syntax_parsing_error\"] == '[Parsing failed]') \n",
    "\n",
    "]\n",
    "print(filtered_df.columns)\n",
    "\n",
    "\n",
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)     \n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Show full content in each cell\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(df[\"error_type\"].unique())\n",
    "\n",
    "\n",
    "errors_count_df = get_error_category_counts(filtered_df, f\"{error_type}_category\")\n",
    "print(errors_count_df)\n",
    "\n",
    "make_categories_pie_chart(errors_count_df, title=f\"{error_type.capitalize()} Error Categories\")\n",
    "make_categories_bar_chart(errors_count_df, title=f\"{error_type.capitalize()} Error Categories\")\n",
    "\n",
    "\n",
    "filtered_df.head(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compariison DF and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../../')  # Add the path to the my_packages module\n",
    "os.environ['EXPERIMENT_DB_NAME'] = \"few_shot_experiments\"\n",
    "\n",
    "from my_packages.analysis.error_analysis import (\n",
    "    categorize_semantic_errors, \n",
    "    categorize_syntax_error, \n",
    "    categorize_syntax_parsing_error,\n",
    "    categorize_test_errors, \n",
    "    extract_semantic_errors, \n",
    "    extract_test_error, \n",
    "    get_error_category_counts, \n",
    "    make_categories_bar_chart, \n",
    "    make_categories_pie_chart\n",
    ")\n",
    "from my_packages.db_service.error_service import delete_error_docs, errors_to_df, pretty_print_errors\n",
    "from my_packages.evaluation.midio_compiler import compile_code, is_code_syntax_valid\n",
    "from my_packages.db_service.experiment_service import experiment_exists, pretty_print_experiment_collections, run_experiment_quality_checks, setup_experiment_collection\n",
    "\n",
    "def categorize_erors(experiment_name: str,\n",
    "                     metric: str,\n",
    "                     filter: dict = {\n",
    "                         \"eval_method\": \"3_fold\",\n",
    "                         \"model_name\": \"llama3.2:3b-instruct-fp16\",\n",
    "                         \"phase\": \"testing\",\n",
    "                     },\n",
    "                     ):\n",
    "    # Categorize and extract errors\n",
    "    df = errors_to_df(experiment_name, filter=filter)\n",
    "    if metric == \"syntax\":\n",
    "        df[\"syntax_error\"] = df[\"stderr\"]\n",
    "        df[\"syntax_parsing_error\"] = df[\"stderr\"].apply(categorize_syntax_parsing_error)\n",
    "        df[\"syntax_category\"] = df[\"stderr\"].apply(categorize_syntax_error)\n",
    "    elif metric == \"semantic\":\n",
    "        df[\"semantic_error\"] = df[\"error_msg\"].apply(extract_semantic_errors)\n",
    "        df[\"semantic_category\"] = df[\"error_msg\"].apply(categorize_semantic_errors)\n",
    "    elif metric == \"tests\":\n",
    "        df[\"tests_category\"] = df[\"test_result\"].apply(categorize_test_errors)\n",
    "        df[\"tests_error\"] = df.apply(\n",
    "            lambda row: extract_test_error(row[\"tests_category\"], row[\"error_msg\"], row[\"test_result\"]),\n",
    "            axis=1\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def get_all_errors_comparison(metric, parsing_errors = True):\n",
    "    rows = []\n",
    "    for model in [\"llama3.2:3b-instruct-fp16\", \"llama3.3:70b-instruct-fp16\", \"phi4:14b-fp16\"]:\n",
    "        for prompt in [\"regular\", \"signature\"]:\n",
    "            for shots in [5]:\n",
    "                df = categorize_erors(\n",
    "                    experiment_name=f\"{prompt}_similarity_{shots}_shot\",\n",
    "                    metric=metric,\n",
    "                    filter={\"eval_method\": \"3_fold\", \"model_name\": model, \"phase\": \"testing\"},\n",
    "                )\n",
    "                filtered_df = df[df[\"error_type\"] == metric]\n",
    "                if parsing_errors:\n",
    "                    filtered_df = filtered_df[filtered_df[\"syntax_category\"] == \"Parsing failed\"]\n",
    "                    errors_count_df = get_error_category_counts(filtered_df, f\"{metric}_parsing_error\")\n",
    "                else:\n",
    "                    errors_count_df = get_error_category_counts(filtered_df, f\"{metric}_category\")\n",
    "            \n",
    "                # Loop over each error category row and append as individual rows\n",
    "                # make_categories_pie_chart(errors_count_df, title=f\"{error_type.capitalize()} Error Categories\")\n",
    "                # make_categories_bar_chart(errors_count_df, title=f\"{error_type.capitalize()} Error Categories\")\n",
    "                for _, row in errors_count_df.iterrows():\n",
    "                    rows.append({\n",
    "                        \"model\": model,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"shots\": shots,\n",
    "                        \"category\": row[\"category\"],\n",
    "                        \"count\": row[\"count\"],\n",
    "                        \"percentage\": row[\"percentage\"],\n",
    "                    })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "comparison_df = get_all_errors_comparison(\"syntax\")\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(9, 4.2))  # Slightly shorter height\n",
    "models = [\"llama3.2:3b-instruct-fp16\", \"phi4:14b-fp16\", \"llama3.3:70b-instruct-fp16\"]\n",
    "prompts = [\"regular\", \"signature\"]\n",
    "\n",
    "for row_idx, prompt in enumerate(prompts):\n",
    "    for col_idx, model in enumerate(models):\n",
    "        ax = axs[row_idx, col_idx]\n",
    "        df = comparison_df[\n",
    "            (comparison_df[\"prompt\"] == prompt) & (comparison_df[\"model\"] == model)\n",
    "        ]\n",
    "\n",
    "        df = df.sort_values(by=\"percentage\", ascending=False).head(5)\n",
    "        df = df.sort_values(by=\"percentage\", ascending=True)\n",
    "\n",
    "        bars = ax.barh(df[\"category\"], df[\"percentage\"], color=\"#aed6f1\", edgecolor=\"gray\")\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            ax.text(width + 0.6, bar.get_y() + bar.get_height() / 2,\n",
    "                    f\"{width:.1f}\", va=\"center\", fontsize=5)\n",
    "\n",
    "        # Enhance title\n",
    "        model_label = model.upper()\n",
    "        ax.set_title(model_label, fontsize=7, fontweight='bold')\n",
    "        ax.set_xlim(0, 50)\n",
    "        ax.set_xlabel(\"Percentage\", fontsize=6)\n",
    "        ax.set_ylabel(f\"({prompt.capitalize()} prompt)\\n Unexpected Tokens\", fontsize=6) if col_idx == 0 else ax.set_ylabel(\"\")\n",
    "        ax.tick_params(axis='both', labelsize=5)\n",
    "        ax.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.60)\n",
    "# plt.tight_layout(pad=0.1)\n",
    "plt.savefig(\"NY_top5_unexpected_tokens_all_models_compact.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
