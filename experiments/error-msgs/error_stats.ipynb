{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "REGULAR prompts:\n",
      "\n",
      "\n",
      "    PHI4:14B-FP16 model:\n",
      "\n",
      "         category  count  percentage\n",
      "0  Parsing failed    804       100.0\n",
      "                  category  count  percentage\n",
      "0  Symbol Resolution Error   2281   47.789650\n",
      "1    Type Resolution Error   1918   40.184370\n",
      "2       Invalid connection    370    7.751938\n",
      "3     Other Semantic Error    102    2.137021\n",
      "4         Duplicate symbol     86    1.801802\n",
      "5  Invalid function header     16    0.335219\n",
      "\n",
      "    LLAMA3.2:3B-INSTRUCT-FP16 model:\n",
      "\n",
      "              category  count  percentage\n",
      "0       Parsing failed    954   98.452012\n",
      "1  Tokenization failed     15    1.547988\n",
      "                     category  count  percentage\n",
      "0     Symbol Resolution Error    955   47.917712\n",
      "1       Type Resolution Error    850   42.649272\n",
      "2          Invalid connection     94    4.716508\n",
      "3     Invalid function header     34    1.705971\n",
      "4            Duplicate symbol     28    1.404917\n",
      "5        Other Semantic Error     14    0.702459\n",
      "6       Invalid AST structure     10    0.501756\n",
      "7       Compiler plugin error      6    0.301054\n",
      "8  Expected function or event      2    0.100351\n",
      "\n",
      "    LLAMA3.3:70B-INSTRUCT-FP16 model:\n",
      "\n",
      "         category  count  percentage\n",
      "0  Parsing failed    540       100.0\n",
      "                  category  count  percentage\n",
      "0  Symbol Resolution Error   1103   49.886929\n",
      "1    Type Resolution Error    916   41.429218\n",
      "2       Invalid connection    122    5.517865\n",
      "3  Invalid function header     60    2.713704\n",
      "4    Invalid AST structure      6    0.271370\n",
      "5     Other Semantic Error      4    0.180914\n",
      "\n",
      "\n",
      "SIGNATURE prompts:\n",
      "\n",
      "\n",
      "    PHI4:14B-FP16 model:\n",
      "\n",
      "              category  count  percentage\n",
      "0       Parsing failed   1560   97.744361\n",
      "1  Tokenization failed     36    2.255639\n",
      "                  category  count  percentage\n",
      "0       Invalid connection    290   33.180778\n",
      "1  Symbol Resolution Error    186   21.281465\n",
      "2    Type Resolution Error    184   21.052632\n",
      "3     Other Semantic Error    100   11.441648\n",
      "4         Duplicate symbol     90   10.297483\n",
      "5  Invalid function header     24    2.745995\n",
      "  category  count  percentage\n",
      "0      0/3    294   76.762402\n",
      "1      1/3     63   16.449086\n",
      "2      2/3     26    6.788512\n",
      "\n",
      "    LLAMA3.2:3B-INSTRUCT-FP16 model:\n",
      "\n",
      "              category  count  percentage\n",
      "0       Parsing failed   1080   95.238095\n",
      "1  Tokenization failed     54    4.761905\n",
      "                  category  count  percentage\n",
      "0  Symbol Resolution Error    246   38.862559\n",
      "1    Type Resolution Error    152   24.012638\n",
      "2       Invalid connection    148   23.380727\n",
      "3     Other Semantic Error     35    5.529226\n",
      "4         Duplicate symbol     26    4.107425\n",
      "5  Invalid function header     26    4.107425\n",
      "  category  count  percentage\n",
      "0      0/3    260   75.801749\n",
      "1      1/3     67   19.533528\n",
      "2      2/3     16    4.664723\n",
      "\n",
      "    LLAMA3.3:70B-INSTRUCT-FP16 model:\n",
      "\n",
      "              category  count  percentage\n",
      "0       Parsing failed    885   99.662162\n",
      "1  Tokenization failed      3    0.337838\n",
      "                  category  count  percentage\n",
      "0  Symbol Resolution Error     94   34.057971\n",
      "1       Invalid connection     80   28.985507\n",
      "2  Invalid function header     52   18.840580\n",
      "3    Type Resolution Error     34   12.318841\n",
      "4     Other Semantic Error     16    5.797101\n",
      "  category  count  percentage\n",
      "0      0/3    107   84.920635\n",
      "1      2/3     15   11.904762\n",
      "2      1/3      4    3.174603\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sys.path.append('../../')  # Add the path to the my_packages module\n",
    "os.environ['EXPERIMENT_DB_NAME'] = \"context_experiments\"\n",
    "from my_packages.analysis.error_analysis import categorize_semantic_errors, categorize_syntax_error, categorize_syntax_parsing_error, categorize_test_errors, extract_semantic_errors, extract_test_error, get_error_category_counts, make_categories_bar_chart, make_categories_pie_chart\n",
    "from my_packages.db_service.error_service import delete_error_docs, errors_to_df, pretty_print_errors\n",
    "from my_packages.evaluation.midio_compiler import compile_code, is_code_syntax_valid\n",
    "from my_packages.db_service.experiment_service import experiment_exists, pretty_print_experiment_collections, run_experiment_quality_checks, setup_experiment_collection\n",
    "\n",
    "experiment_name = f\"RAG_5_shot\"\n",
    "prompt_types = [\"regular\", \"signature\"]\n",
    "models = [\"phi4:14b-fp16\", \"llama3.2:3b-instruct-fp16\", \"llama3.3:70b-instruct-fp16\"]\n",
    "\n",
    "\n",
    "# Categorize and extract errors:\n",
    "\n",
    "for prompt in prompt_types:\n",
    "    print(f\"\\n\\n{prompt.upper()} prompts:\\n\")\n",
    "    for model in models:\n",
    "        print(f\"\\n    {model.upper()} model:\\n\")    \n",
    "        filter = {\n",
    "            \"eval_method\": \"3_fold\",\n",
    "            \"model_name\": model,\n",
    "            \"phase\": \"testing\", \n",
    "        }\n",
    "        df = errors_to_df(f\"{prompt}_{experiment_name}\", filter=filter)\n",
    "        # print(df.count())\n",
    "        df[\"syntax_error\"] = df[\"stderr\"]\n",
    "        df[\"syntax_parsing_error\"] = df[\"stderr\"].apply(categorize_syntax_parsing_error)\n",
    "        df[\"syntax_category\"] = df[\"stderr\"].apply(categorize_syntax_error)\n",
    "\n",
    "        df[\"semantic_error\"] = df[\"error_msg\"].apply(extract_semantic_errors)\n",
    "        df[\"semantic_category\"] = df[\"error_msg\"].apply(categorize_semantic_errors)\n",
    "\n",
    "        df[\"tests_category\"] = df[\"test_result\"].apply(categorize_test_errors)\n",
    "        df[\"tests_error\"] = df.apply(\n",
    "            lambda row: extract_test_error(row[\"tests_category\"], row[\"error_msg\"], row[\"test_result\"]),\n",
    "            axis=1\n",
    "        )\n",
    "        error_type = [\"syntax\", \"semantic\"] if prompt == \"regular\" else [\"syntax\", \"semantic\", \"tests\"]\n",
    "\n",
    "        for error_type in error_type:\n",
    "            filtered_df = df[\n",
    "                (df[\"error_type\"] == error_type) \n",
    "                # & (df[f\"{error_type}_category\"] == {'Symbol Resolution Error'})\n",
    "                # & (df[f\"syntax_parsing_error\"] == '[Parsing failed]') \n",
    "\n",
    "            ]\n",
    "\n",
    "            errors_count_df = get_error_category_counts(filtered_df, f\"{error_type}_category\")\n",
    "            print(errors_count_df)\n",
    "\n",
    "            # # Show all rows\n",
    "            # pd.set_option('display.max_rows', None)     \n",
    "\n",
    "            # # Show all columns\n",
    "            # pd.set_option('display.max_columns', None)\n",
    "\n",
    "            # # Show full content in each cell\n",
    "            # pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "            # print(df[\"error_type\"].unique())\n",
    "\n",
    "# make_categories_pie_chart(errors_count_df, title=f\"{error_type.capitalize()} Error Categories\")\n",
    "# make_categories_bar_chart(errors_count_df, title=f\"{error_type.capitalize()} Error Categories\")\n",
    "\n",
    "\n",
    "# filtered_df.head(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax error comparrison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy error collection to another db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection regular_few_shot_5_shot_errors already exists in context_experiments DB. Skipping.\n",
      "Collection signature_few_shot_5_shot_errors already exists in context_experiments DB. Skipping.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "sys.path.append('../../')  # Add the path to the my_packages module\n",
    "\n",
    "from my_packages.db_service import get_db_connection\n",
    "\n",
    "from_experiment = \"few_shot\"\n",
    "to_experiment = \"context\"\n",
    "source_db = get_db_connection(f\"{from_experiment}_experiments\")\n",
    "target_db = get_db_connection(f\"{to_experiment}_experiments\")\n",
    "\n",
    "for prompt in [\"regular\", \"signature\"]:\n",
    "    source_collection = source_db[f\"{prompt}_similarity_5_shot_errors\"]\n",
    "\n",
    "    new_target_collection_name = f\"{prompt}_{from_experiment}_5_shot_errors\"\n",
    "    if new_target_collection_name in target_db.list_collection_names():\n",
    "        print(f\"Collection {new_target_collection_name} already exists in {target_db.name} DB. Skipping.\")\n",
    "        continue\n",
    "    target_collection = target_db[new_target_collection_name]\n",
    "\n",
    "    docs = list(source_collection.find())  # Use list() to avoid cursor exhaustion\n",
    "    if docs:\n",
    "        target_collection.insert_many(docs)\n",
    "        print(f\"Copied {len(docs)} docs to  {new_target_collection_name} in {target_db.name} DB.\")\n",
    "    else:\n",
    "        print(f\"No documents found in {new_target_collection_name} in {target_db.name} DB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compariison DF and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Collection 'regular_baseline_5_shot_errors' is empty in MongoDB.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 109\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# plt.rcParams.update({\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#     \"font.size\": 10,              # Base font size\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#     \"axes.titlesize\": 10,        # subplot titles (if any)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#     \"figure.titlesize\": 12       # suptitle\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# })\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 109\u001b[0m      comparison_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_errors_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msyntax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparsing_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m      rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(models)\n\u001b[1;32m    112\u001b[0m      cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(experiment_types)\n",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m, in \u001b[0;36mget_all_errors_comparison\u001b[0;34m(metric, models, experiment_types, prompt, shot, parsing_errors)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m experiment_type \u001b[38;5;129;01min\u001b[39;00m experiment_types:\n\u001b[0;32m---> 69\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mcategorize_erors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprompt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexperiment_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mshot\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_shot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3_fold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtesting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m         filtered_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m metric]\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsing_errors:\n",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m, in \u001b[0;36mcategorize_erors\u001b[0;34m(experiment_name, metric, filter)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcategorize_erors\u001b[39m(experiment_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     45\u001b[0m                      metric: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     46\u001b[0m                      \u001b[38;5;28mfilter\u001b[39m: \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m     47\u001b[0m                     ):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Categorize and extract errors\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43merrors_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msyntax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     51\u001b[0m         df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msyntax_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Thesis_project/experiments/error-msgs/../../my_packages/db_service/error_service.py:120\u001b[0m, in \u001b[0;36merrors_to_df\u001b[0;34m(experiment, model, filter)\u001b[0m\n\u001b[1;32m    117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(collection\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;28mfilter\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}))\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is empty in MongoDB.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "\u001b[0;31mValueError\u001b[0m: Collection 'regular_baseline_5_shot_errors' is empty in MongoDB."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "experiment = \"syncode_experiments\"\n",
    "\n",
    "experiment_types = [\"baseline\", \"RAG\"]\n",
    "color_map = {\n",
    "    \"baseline\": \"#aed6f1\",\n",
    "    \"RAG\": \"#b0e57c\",\n",
    "    # \"full-context\": \"#b0e57c\",\n",
    "}\n",
    "#aed6f1\n",
    "models = [\"llama3.2:3b-instruct-fp16\", \"phi4:14b-fp16\", \"llama3.3:70b-instruct-fp16\"]\n",
    "prompts = [\"regular\", \"signature\"]\n",
    "shot = 5\n",
    "\n",
    "os.environ['EXPERIMENT_DB_NAME'] = experiment\n",
    "\n",
    "sys.path.append('../../')  # Add the path to the my_packages module\n",
    "from my_packages.analysis.error_analysis import (\n",
    "    categorize_semantic_errors, \n",
    "    categorize_syntax_error, \n",
    "    categorize_syntax_parsing_error,\n",
    "    categorize_test_errors, \n",
    "    extract_semantic_errors, \n",
    "    extract_test_error, \n",
    "    get_error_category_counts, \n",
    "    make_categories_bar_chart, \n",
    "    make_categories_pie_chart\n",
    ")\n",
    "from my_packages.db_service.error_service import delete_error_docs, errors_to_df, pretty_print_errors\n",
    "from my_packages.evaluation.midio_compiler import compile_code, is_code_syntax_valid\n",
    "from my_packages.db_service.experiment_service import experiment_exists, pretty_print_experiment_collections, run_experiment_quality_checks, setup_experiment_collection\n",
    "\n",
    "def categorize_erors(experiment_name: str,\n",
    "                     metric: str,\n",
    "                     filter: dict\n",
    "                    ):\n",
    "    # Categorize and extract errors\n",
    "    df = errors_to_df(experiment_name, filter=filter)\n",
    "    if metric == \"syntax\":\n",
    "        df[\"syntax_error\"] = df[\"stderr\"]\n",
    "        df[\"syntax_parsing_error\"] = df[\"stderr\"].apply(categorize_syntax_parsing_error)\n",
    "        df[\"syntax_category\"] = df[\"stderr\"].apply(categorize_syntax_error)\n",
    "    elif metric == \"semantic\":\n",
    "        df[\"semantic_error\"] = df[\"error_msg\"].apply(extract_semantic_errors)\n",
    "        df[\"semantic_category\"] = df[\"error_msg\"].apply(categorize_semantic_errors)\n",
    "    elif metric == \"tests\":\n",
    "        df[\"tests_category\"] = df[\"test_result\"].apply(categorize_test_errors)\n",
    "        df[\"tests_error\"] = df.apply(\n",
    "            lambda row: extract_test_error(row[\"tests_category\"], row[\"error_msg\"], row[\"test_result\"]),\n",
    "            axis=1\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def get_all_errors_comparison(metric, models, experiment_types, prompt, shot, parsing_errors = True):\n",
    "    rows = []\n",
    "    for model in models:\n",
    "        for experiment_type in experiment_types:\n",
    "            df = categorize_erors(\n",
    "                experiment_name=f\"{prompt}_{experiment_type}_{shot}_shot\",\n",
    "                metric=metric,\n",
    "                filter={\"eval_method\": \"3_fold\", \"model_name\": model, \"phase\": \"testing\"},\n",
    "            )\n",
    "            filtered_df = df[df[\"error_type\"] == metric]\n",
    "            if parsing_errors:\n",
    "                filtered_df = filtered_df[filtered_df[\"syntax_category\"] == \"Parsing failed\"]\n",
    "                errors_count_df = get_error_category_counts(filtered_df, f\"{metric}_parsing_error\")\n",
    "            else:\n",
    "                errors_count_df = get_error_category_counts(filtered_df, f\"{metric}_category\")\n",
    "        \n",
    "            # Loop over each error category row and append as individual rows\n",
    "            # make_categories_pie_chart(errors_count_df, title=f\"{error_type.capitalize()} Error Categories\")\n",
    "            # make_categories_bar_chart(errors_count_df, title=f\"{error_type.capitalize()} Error Categories\")\n",
    "            for _, row in errors_count_df.iterrows():\n",
    "                rows.append({\n",
    "                    \"model\": model,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"experiment_type\": experiment_type,\n",
    "                    \"shots\": shot,\n",
    "                    \"category\": row[\"category\"],\n",
    "                    \"count\": row[\"count\"],\n",
    "                    \"percentage\": row[\"percentage\"],\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if  \"full-context\" in experiment_types and \"phi4:14b-fp16\" in models:\n",
    "    models.remove(\"phi4:14b-fp16\")\n",
    "\n",
    "# plt.rcParams.update({\n",
    "#     \"font.size\": 10,              # Base font size\n",
    "#     \"axes.titlesize\": 10,        # subplot titles (if any)\n",
    "#     \"axes.labelsize\": 8,         # x/y labels\n",
    "#     \"xtick.labelsize\": 10,        # tick labels\n",
    "#     \"ytick.labelsize\": 10,\n",
    "#     \"legend.fontsize\": 10,       # legend text\n",
    "#     \"figure.titlesize\": 12       # suptitle\n",
    "# })\n",
    "for prompt in prompts:\n",
    "     comparison_df = get_all_errors_comparison(\"syntax\", models, experiment_types, prompt, shot, parsing_errors=True)\n",
    "\n",
    "     rows = len(models)\n",
    "     cols = len(experiment_types)\n",
    "     fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*2))  # Slightly shorter height\n",
    "\n",
    "     for row_idx, model in enumerate(models):\n",
    "         for col_idx, experiment_type in enumerate(experiment_types):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            df = comparison_df[\n",
    "                 (comparison_df[\"experiment_type\"] == experiment_type) & (comparison_df[\"model\"] == model)\n",
    "             ]\n",
    "\n",
    "            df = df.sort_values(by=\"percentage\", ascending=False).head(5)\n",
    "            df = df.sort_values(by=\"percentage\", ascending=True)\n",
    "\n",
    "            bars = ax.barh(\n",
    "                 df[\"category\"],\n",
    "                 df[\"percentage\"],\n",
    "                 color=color_map[experiment_type],\n",
    "                 edgecolor=\"gray\"\n",
    "             )\n",
    "\n",
    "            for bar in bars:\n",
    "                 width = bar.get_width()\n",
    "                 ax.text(\n",
    "                     width + 0.6,\n",
    "                     bar.get_y() + bar.get_height()/2,\n",
    "                     f\"{width:.1f}\",\n",
    "                     va=\"center\",\n",
    "                     fontsize=6,\n",
    "                 )\n",
    "             # Enhance title\n",
    "            model_label = model.capitalize().removesuffix(\"-FP16\")\n",
    "            ax.set_title(model_label, fontsize=7, fontweight='bold')\n",
    "            ax.set_xlim(0, 55)\n",
    "            ax.set_xlabel(\"Percentage\", fontsize=6, fontweight='bold')\n",
    "            ax.set_ylabel(f\"Unexpected Tokens\", fontsize=6, fontweight='bold') if col_idx == 0 else ax.set_ylabel(\"\")\n",
    "\n",
    "            ax.tick_params(axis='both', labelsize=6)\n",
    "            ax.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "     # Adjust spacing between subplots\n",
    "     fig.suptitle(f\"Top 5 Unexpected Tokens for {prompt.capitalize()} Prompt\\n\", fontsize=10, fontweight='bold')\n",
    "     legend_handles = [\n",
    "         mpatches.Patch(color=color, label=(\"SynCode\" if label == \"RAG\" else \"Baseline\"))\n",
    "         for label, color in color_map.items()\n",
    "     ]\n",
    "\n",
    "     fig.legend(\n",
    "         handles=legend_handles,\n",
    "         title=\"\",\n",
    "         loc='upper left',\n",
    "         bbox_to_anchor=(0.01, 0.99),  # Fine-tune as needed\n",
    "         fontsize=8,\n",
    "         title_fontsize=8,\n",
    "     )\n",
    "\n",
    "     plt.subplots_adjust(wspace=0.6, hspace=0.6, top=0.85)\n",
    "\n",
    "     # plt.tight_layout(pad=0.1)\n",
    "\n",
    "     plt.savefig(\n",
    "         f\"{prompt}_top5_unexpected_tokens_all_models_compact.png\", \n",
    "         dpi=400,\n",
    "         bbox_inches=\"tight\",    # ← expand the saved region to include all artists\n",
    "         pad_inches=0.1          # ← a little padding around the edges\n",
    ")\n",
    "     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
