{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from my_packages.evaluation.metrics import estimate_pass_at_k\n",
    "from my_packages.prompting.few_shot import create_few_shot_prompt, create_final_node_prompt, get_semantic_similarity_example_selector\n",
    "from my_packages.utils.server_utils import server_diagnostics\n",
    "from my_packages.evaluation.models import invoke_anthropic_model, invoke_openai_model, invoke_o1_model, invoke_ollama_model\n",
    "from colorama import Fore, Style\n",
    "from sklearn.metrics import f1_score\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client, evaluate\n",
    "from langsmith.schemas import Example, Run\n",
    "from langsmith.beta import convert_runs_to_test\n",
    "from sklearn.metrics import f1_score\n",
    "from langsmith.schemas import Example\n",
    "\n",
    "def f1_score_summary_evaluator(outputs: list[dict], reference_outputs: list[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Computes the mean F1 score across all examples.\n",
    "    \n",
    "    Parameters:\n",
    "    - outputs: List of input examples.\n",
    "    - reference_outputs: List of corresponding output examples.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Summary containing the mean F1 score.\n",
    "    \"\"\"\n",
    "\n",
    "    f1_scores = []\n",
    "    print(outputs)\n",
    "    for target, response in zip(outputs, reference_outputs):\n",
    "    \n",
    "        true_response = set(target[\"response\"].replace(\",\", \"\").split())\n",
    "        predicted_response = set(response[\"response\"].replace(\",\", \"\").split())\n",
    "\n",
    "        # Combine all nodes to ensure correct alignment\n",
    "        all_nodes = sorted(true_response.union(predicted_response))\n",
    "        y_true = [1 if node in true_response else 0 for node in all_nodes]\n",
    "        y_pred = [1 if node in predicted_response else 0 for node in all_nodes]\n",
    "\n",
    "        # Compute F1 score for this example\n",
    "        score = f1_score(y_true, y_pred)\n",
    "        f1_scores.append(score)\n",
    "\n",
    "    # Compute mean F1 score\n",
    "    mean_f1_score = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0\n",
    "\n",
    "    return {\"key\": \"mean_f1_score\", \"score\": mean_f1_score}\n",
    "\n",
    "\n",
    "def extract_nodes(response_text: str):\n",
    "    \"\"\"Extract nodes from the response using regex.\"\"\"\n",
    "    match = re.search(fr\"```midio(.*?)```\", response_text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else response_text.strip()\n",
    "\n",
    "# def calculate_pass_at_k_scores(target, response):\n",
    "#     \"\"\"Computes Pass@K for LangSmith evaluation.\"\"\"\n",
    "#     print(\"Output: \", target)\n",
    "#     result_dict = {response.outputs[\"response\"]: target.outputs[\"response\"]}\n",
    "#     pass_at_k = estimate_pass_at_k(result_dict, ks=[1, 3, 5])  # Adjust `ks` as needed\n",
    "#     return pass_at_k.mean()  # Return average score for LangSmith tracking\n",
    "\n",
    "def calculate_f1_score(target: Example, response: Example):\n",
    "    \"\"\"Computes F1 score for LangSmith evaluation.\"\"\"\n",
    "\n",
    "    true_response = set(response.outputs[\"response\"].replace(\",\", \"\").split())\n",
    "    predicted_response = set( target.outputs[\"response\"].replace(\",\", \"\").split())\n",
    "\n",
    "    # Compute F1 score using sklearn\n",
    "    all_nodes = sorted(true_response.union(predicted_response))\n",
    "    y_true = [1 if node in true_response else 0 for node in all_nodes]\n",
    "    y_pred = [1 if node in predicted_response else 0 for node in all_nodes]\n",
    "\n",
    "    return f1_score(y_true, y_pred)\n",
    "\n",
    "def generate_response(client, model, task, available_nodes, example_pool):\n",
    "    \"\"\"Generates response using the model and selected few-shot examples.\"\"\"\n",
    "    similar_examples = example_pool.select_examples({\"task\": task})\n",
    "    few_shot = create_few_shot_prompt(similar_examples, \"NODES_TEMPLATE\")\n",
    "    final_prompt_template = create_final_node_prompt(few_shot, \"NODE_GENERATOR_TEMPLATE\", \"NODES_TEMPLATE\")\n",
    "    prompt = final_prompt_template.format(task=task, external_functions=available_nodes)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "\n",
    "    llm = client(\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        num_predict=256,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        stream=False,\n",
    "        num_ctx=10000,\n",
    "        stop=[\"```<|eot_id|>\"],\n",
    "    )\n",
    "    print(\"generating repsonse ..\")\n",
    "    chain = (final_prompt_template | llm)\n",
    "    response = chain.invoke({\"task\": task, \"external_functions\": available_nodes}, {\"run_name\": \"Node Prediction\"})\n",
    "\n",
    "    return {\"response\": extract_nodes(response.content)}  # If string retuned, LangSmith will convert it to an dict with 'output' as key\n",
    "\n",
    "def evaluate_nodes(client, model, available_nodes, example_pool):\n",
    "    \"\"\"Runs model evaluation using LangSmith `evaluate()`.\"\"\"\n",
    "    results = evaluate(\n",
    "        lambda inputs: generate_response(client, model, inputs[\"task\"], available_nodes, example_pool),\n",
    "        data=[test_data[0]], #client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),\n",
    "        evaluators=[\n",
    "            # calculate_pass_at_k_scores,\n",
    "            calculate_f1_score\n",
    "        ],\n",
    "        summary_evaluators= [\n",
    "            f1_score_summary_evaluator\n",
    "        ],\n",
    "        experiment_prefix=\"Node Prediction Evaluation\",\n",
    "        upload_results=True\n",
    "    )\n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "selector = get_semantic_similarity_example_selector(\n",
    "    [example_to_dict(example) for example in train_data], \n",
    "    embed_client(model=models[0]),\n",
    "    shots=5,\n",
    "    input_keys=[\"task\"],\n",
    ")\n",
    "client = ChatOllama\n",
    "langsmith_client = Client()\n",
    "evaluate_nodes(client, models[0], available_nodes, selector)\n",
    "\n",
    "# langsmith_client.create_run(\n",
    "#     name=\"ExampleRun\",\n",
    "#     run_type=\"chain\",\n",
    "#     inputs={\"input\": \"Test input\"},\n",
    "#     outputs={\"output\": \"Tesst output\"},\n",
    "#     project_name=\"Thesis_project\",\n",
    "#     end_time=datetime.datetime.now(),\n",
    "# )\n",
    "\n",
    "# convert_runs_to_test(\n",
    "    \n",
    "# )\n",
    "# Select runs named \"extractor\" whose root traces received good feedback\n",
    "# runs = client.list_runs(\n",
    "#     project_name=\"<your_project>\",\n",
    "#     filter='eq(name, \"extractor\")',\n",
    "#     trace_filter='and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))',\n",
    "# )\n",
    "# runs_as_test(runs, dataset_name=\"Extraction Good\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
