{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605d132a-e3b2-42ee-82a2-50ccefbc9139",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf93b4f-f9b0-4929-bd2c-4bb7688d8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%pip install ollama\n",
    "%pip install openai\n",
    "%pip install transformers\n",
    "%pip install tiktoken\n",
    "%pip install huggingface-hub\n",
    "%pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a3c3c-35ac-4f7f-836b-ed72851c222e",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50df78-1de1-470c-b5aa-10c69228ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.append('../')  # Add the path to the my_packages module\n",
    "from my_packages.data_processing.split_dataset import split_on_shots, split\n",
    "from my_packages.data_processing.get_labels_data import used_libraries_from_dataset\n",
    "from my_packages.analysis.analyze_datasets import analyze_library_distribution, analyze_instance_distribution, analyze_visual_node_types_distribution\n",
    "\n",
    "def used_libraries_to_string(data):\n",
    "    name_doc_string = \"\"\n",
    "    for func in data:\n",
    "        name_doc_string += f\"Name: {func['function_name']}\\nDocumentation: {func['doc']}\\n\\n\"\n",
    "    return name_doc_string\n",
    "\n",
    "main_dataset_folder = '../data/mbpp_transformed_code_examples/sanitized-MBPP-midio.json'\n",
    "\n",
    "# main dataset\n",
    "with open(main_dataset_folder, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "    \n",
    "num_shot = 10 # Few-shot examples\n",
    "eval_size_percentage = 0.5\n",
    "train_data, val_data, test_data = split_on_shots(num_shot, eval_size_percentage, dataset, seed = 64, write_to_file=True)\n",
    "\n",
    "# Extract all unique nodes (library_functions) across datasets\n",
    "used_libraries_json = used_libraries_from_dataset(train_data)\n",
    "explained_used_libraries = used_libraries_to_string(used_libraries_json)\n",
    "\n",
    "#Bar chart of distribuation\n",
    "analyze_library_distribution(train_data, val_data, test_data)\n",
    "analyze_instance_distribution(train_data, val_data, test_data)\n",
    "analyze_visual_node_types_distribution(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b4428-6b24-441c-90da-44e03336f79d",
   "metadata": {},
   "source": [
    "## Prompt utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ed17d-7b3a-469b-8da6-bfb763f9e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "def extract_nodes(response_text):\n",
    "    \"\"\"Extract nodes from the response using regex.\"\"\"\n",
    "    # Match content between ```language and ```\n",
    "    match = re.search(fr\"```midio(.*?)```\", response_text, re.DOTALL)\n",
    "\n",
    "    # Extract and clean up the nodes\n",
    "    if match:\n",
    "        return match.group(1).strip()  # Return only the midio block\n",
    "\n",
    "    # If no match, assume the response might already be nodes without markdown formatting\n",
    "    return response_text.strip()\n",
    "\n",
    "def read_file(_file):\n",
    "    with open(_file) as reader:\n",
    "        return reader.read()\n",
    "\n",
    "def get_prompt_responses(data: json):\n",
    "    script_path = os.path.dirname(os.getcwd())\n",
    "    prompt_template_path = os.path.join(script_path, 'templates/prompts/FEW_SHOT_NODES_TEMPLATE.file')\n",
    "    node_template_path = os.path.join(script_path, 'templates/responses/NODES_TEMPLATE.file')\n",
    "\n",
    "    if not (os.path.exists(prompt_template_path)):\n",
    "        print(\"Few_shot_template.file not found!!\")\n",
    "        return\n",
    "    if not (os.path.exists(node_template_path)):\n",
    "        print(\"Nodes_template.file not found!!\")\n",
    "        return\n",
    "    prompt_template = read_file(prompt_template_path)\n",
    "    node_template = read_file(node_template_path)\n",
    "\n",
    "    prompts = [prompt_template.format(task_description=task['prompts'][0]) for task in data]\n",
    "    responses = [node_template.format(nodes=(\", \".join(task['library_functions']))) for task in data]\n",
    "    return (prompts, responses)\n",
    "\n",
    "def create_few_shot_prompt(train_prompts, train_responses):\n",
    "    guiding = \"You are an expert node selector for the flow-based language Midio. You will get prompted with a task and you will respond with the built-in nodes (aka library functions) that can be used to solve the task.\\n\" \n",
    "    node_list = f\"Only use the following nodes (library functions):\\n {explained_used_libraries}\\n\\n\"\n",
    "    context = guiding + node_list\n",
    "    few_shots_messages = [{\"role\": \"developer\", \"content\": context}] \n",
    "    for i, (prompt, response) in enumerate(zip(train_prompts, train_responses)):\n",
    "        few_shots_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        few_shots_messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return few_shots_messages\n",
    "\n",
    "train_prompts, train_responses = get_prompt_responses(train_data)\n",
    "val_prompts, val_responses = get_prompt_responses(val_data)\n",
    "test_prompts, test_responses = get_prompt_responses(test_data)\n",
    "\n",
    "few_shot_messages = create_few_shot_prompt(train_prompts, train_responses)\n",
    "print(few_shot_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4118592b",
   "metadata": {},
   "source": [
    "## Models to test and Calculations of max tokens\n",
    "\n",
    "1. Define models\n",
    "2. Loop through all responses for train, validation and test data, created from get_prompt_responses() function in previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7821a",
   "metadata": {},
   "source": [
    "### Open source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03057260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from my_packages.utils.tokens import find_max_tokens_nodes_tokenizer, find_max_tokens_nodes_api\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "open_models = [\n",
    "    {\"name\": \"llama3.1:8b\"},\n",
    "    # {\"name\": \"llama3.3:70b-instruct-fp16\", \"tokenization\": \"meta-llama/Meta-Llama-3.1-8B\"},\n",
    "    # {\"name\": \"llama3.3:70b-instruct-q8_0\", \"tokenization\": \"meta-llama/Meta-Llama-3.1-8B\"},\n",
    "    # {\"hg_name\": \"mistralai/Mistral-Small-Instruct-2409\", \"ollama_name\": \"\"},\n",
    "    # {\"hg_name\": \"meta-llama/Llama-3.3-70B-Instruct\", \"ollama_name\": \"\"},\n",
    "    # {\"hg_name\": \"meta-llama/CodeLlama-70b-Instruct-hf\", \"ollama_name\": \"\"},\n",
    "    # {\"hg_name\": \"meta-llama/Llama-3.2-90B-Vision-Instruct\", \"ollama_name\": \"\"}\n",
    "]\n",
    "\n",
    "DATA_DIR = '../data/mbpp_transformed_code_examples/only_files'\n",
    "OUTPUT_JSON = 'token_counts.json'\n",
    "host = 'http://localhost:11434'\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=host+'/v1/',\n",
    "  api_key='ollama' #Ignored with 'ollama'. To use GPT models, this key need to be set.\n",
    ")\n",
    "all_responses = train_responses + val_responses + test_responses\n",
    "open_models_to_test = []\n",
    "for model_info in open_models:\n",
    "\n",
    "    max_tokens = find_max_tokens_nodes_api(all_responses, model_info[\"name\"], client.embeddings)\n",
    "    client\n",
    "    model_result = {\n",
    "        \"name\": model_info[\"name\"],\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    open_models_to_test.append(model_result)\n",
    "\n",
    "print(open_models_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577843d",
   "metadata": {},
   "source": [
    "### GPT-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80adb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt_models = [\n",
    "    {\"name\": \"o1-preview\", \"tokenization\": \"o200k_base\"},\n",
    "    {\"name\": \"gpt-4o\", \"tokenization\": \"o200k_base\"},\n",
    "]\n",
    "\n",
    "gpt_models_to_test = []\n",
    "for model_info in gpt_models:\n",
    "    encoding = tiktoken.encoding_for_model(model_info[\"name\"])\n",
    "    max_tokens = find_max_tokens_nodes_tokenizer(all_responses, encoding)\n",
    "    model_result = {\n",
    "        \"name\": model_info[\"name\"],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    gpt_models_to_test.append(model_result)\n",
    "print(gpt_models_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05ea3-4062-48f7-94c9-51c0cfb96505",
   "metadata": {},
   "source": [
    "## Test some Models, with different seeds, temperatures, top_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b5510-e0d2-42ca-825e-0a43eb7e3d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from my_packages.utils.ollama_utils import is_remote_server_reachable\n",
    "import ollama # https://github.com/ollama/ollama-python\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from subprocess import call\n",
    "\n",
    "host = 'http://localhost:11434'\n",
    "\n",
    "if is_remote_server_reachable(host + \"/api/tags\"):\n",
    "    print(\"Server is reachable.\")\n",
    "else:\n",
    "    rc = call(\"../SSH_FORWARDING.sh\")\n",
    "    print(\"Ollama server is not reachable. Batch job might have finished. Try running bash script again.\")\n",
    "\n",
    "ollama.Client(\n",
    "  host=host,\n",
    ")\n",
    "client = OpenAI(\n",
    "  base_url=host+'/v1/',\n",
    "  api_key='ollama' #Ignored with 'ollama'. To use GPT models, this key need to be set.\n",
    ")\n",
    "\n",
    "# Function to generate and evaluate responses\n",
    "def evaluate(client, model, prompts, responses, seed, max_new_tokens=50, temperature=0.7, top_p=0.9):\n",
    "    correct = 0\n",
    "    total = len(prompts)\n",
    "    for index, (prompt, true_response) in enumerate(zip(prompts, responses)):\n",
    "        max_retries = 3\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                print(f\"Generating response..\", end=\"\\r\")\n",
    "                generated = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=few_shot_messages + [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    # template=\"{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\\n\\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\\n\\n```midio\\n{{ .Response }}\\n```<|eot_id|>\", # Template enforces code block\n",
    "                    max_tokens=max_new_tokens,\n",
    "                    seed=seed,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    stream=False,\n",
    "                    stop=[\"```<|eot_id|>\"]  # Ensure the response stops after the code block\n",
    "                )\n",
    "                \n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Attempt {retries} failed with error: {e}\")\n",
    "                if is_remote_server_reachable(host + \"/api/tags\"):\n",
    "                    print(\"Server is reachable.\")\n",
    "                else:\n",
    "                    rc = call(\"../SSH_FORWARDING.sh\")\n",
    "                    # Check the result\n",
    "                    if rc == 0:\n",
    "                        print(\"Command executed successfully!\")\n",
    "                    else:\n",
    "                        print(f\"SSH command failed with return code {rc}\")\n",
    "                        print(f\"Try to manually connect to the server again. \")\n",
    "                        #Wait one minute before trying again\n",
    "                        try:\n",
    "                            for remaining in range(60, 0, -1):\n",
    "                                print(f\"Next try in {remaining} seconds\", end=\"\\r\")  # Print on the same line\n",
    "                                time.sleep(1)  # Wait for 1 second\n",
    "                            print(\"Time's up!                          \\n\")  # Clear the line after completion\n",
    "                        except KeyboardInterrupt:\n",
    "                            print(\"\\nCountdown interrupted!\")\n",
    "    \n",
    "        else:\n",
    "            print(\"Failed to get a response from the server after \" + str(retries) + \" attempts.\")\n",
    "            generated_nodes = \"\"\n",
    "            return -1\n",
    "\n",
    "        # Extract nodes from the generated response and transform to a set\n",
    "        generated_nodes = extract_nodes(generated.choices[0].message.content)\n",
    "        generated_nodes_set = set(generated_nodes.replace(\",\", \"\").split())\n",
    "        # Extract nodes from the true response and transform to a set\n",
    "        true_response_nodes = extract_nodes(true_response)\n",
    "        true_response_set = set(true_response_nodes.replace(\",\", \"\").split())\n",
    "        # Extract available library functions from the used_libraries_json and transform to a set\n",
    "        library_functions = set(item['function_name'] for item in used_libraries_json)\n",
    "\n",
    "        print(f\"\\n\\nSample: {index}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated response:\\n {generated_nodes}\")\n",
    "        print(f\"True response:\\n {true_response}\")\n",
    "\n",
    "        #Remove invalid nodes from the generated nodes\n",
    "        valid_generated_nodes = generated_nodes_set.intersection(library_functions)\n",
    "        \n",
    "        if true_response_set.issubset(valid_generated_nodes):\n",
    "            print(\"correct response\")\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(\"Invalid response\")\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model in open_models_to_test:\n",
    "    print(f\"Model: {model['name']}\")\n",
    "    ollama.pull(model['name'])  # Pull the model from the server\n",
    "\n",
    "    # Validate the model on the validation set with different hyperparameters abd a fixed seed\n",
    "    SEED = 42 #During Validation Phase\n",
    "    print(\"max_tokens in dataset with current pipeline:\", model[\"max_tokens\"])\n",
    "\n",
    "    temperatures = [0.5, 0.7, 0.9]\n",
    "    top_ps = [0.2, 0.5, 1.0]\n",
    "    best_accuracy = 0\n",
    "    best_params = {\"temperature\": 0.7, \"top_p\": 0.9}\n",
    "\n",
    "    for temp in temperatures:\n",
    "        for top_p in top_ps:\n",
    "            accuracy = evaluate (\n",
    "                client,\n",
    "                model['name'],\n",
    "                val_prompts,\n",
    "                val_responses,\n",
    "                SEED,\n",
    "                model[\"max_tokens\"],\n",
    "                temp,\n",
    "                top_p\n",
    "            )\n",
    "            print(f\"Tested with temp={temp} and top_p={top_p}. Gave accuracy={accuracy}\")\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {\"temperature\": temp, \"top_p\": top_p}\n",
    "\n",
    "    print(f\"Best Hyperparameters for {model['name']}: {best_params}, Validation Accuracy: {best_accuracy:.2f}\")\n",
    "    \n",
    "    #Test the model on the test setm with different seeds and the best hyperparameters.\n",
    "    seeds = [3, 75, 346]\n",
    "    for seed in seeds:\n",
    "        print(f\"\\nTesting with Seed: {seed}\")\n",
    "\n",
    "        test_accuracy = evaluate(\n",
    "            client,\n",
    "            model['name'],\n",
    "            test_prompts,\n",
    "            test_responses,\n",
    "            seed,\n",
    "            model[\"max_tokens\"],\n",
    "            temperature=best_params[\"temperature\"],\n",
    "            top_p=best_params[\"top_p\"]\n",
    "        )\n",
    "\n",
    "        print(f\"Test Accuracy for {model['name']}: {test_accuracy:.2f}\")\n",
    "\n",
    "        if model[\"name\"] not in results:\n",
    "            results[model[\"name\"]] = []\n",
    "        results[model[\"name\"]].append({\n",
    "            \"seed\": seed,\n",
    "            \"validation_accuracy\": best_accuracy,\n",
    "            \"test_accuracy\": test_accuracy\n",
    "        })\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for run in metrics:print(f\"  Seed {run['seed']}: Validation Accuracy: {run['validation_accuracy']:.2f}, Test Accuracy: {run['test_accuracy']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
