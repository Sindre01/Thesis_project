{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neulab/docprompting-tldr-gpt-neo-1.3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"neulab/docprompting-tldr-gpt-neo-1.3B\")\n",
    "\n",
    "# prompt template\n",
    "prompt = f\"\"\"{tokenizer.bos_token} Potential manual 0: makepkg - package build utility\n",
    "Potential manual 1: -c, --clean Clean up leftover work files and directories after a successful build.\n",
    "Potential manual 2: -r, --rmdeps Upon successful build, remove any dependencies installed by makepkg during dependency auto-resolution and installation when using -s\n",
    "Potential manual 3: CONTENT_OF_THE_MANUAL_3\n",
    "...\n",
    "Potential manual 10: CONTENT_OF_THE_MANUAL_10\"\"\"\n",
    "prompt += f\"{tokenizer.sep_token} clean up work directories after a successful build {tokenizer.sep_token}\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=5,\n",
    "    max_new_tokens=150,\n",
    "    num_return_sequences=2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "gen_tokens = gen_tokens.reshape(1, -1, gen_tokens.shape[-1])[0][0]\n",
    "# to text and clean\n",
    "gen_code = tokenizer.decode(gen_tokens)\n",
    "gen_code = gen_code.split(tokenizer.sep_token)[2].strip().split(tokenizer.eos_token)[0].strip()\n",
    "print(gen_code)\n",
    "\n",
    ">>> makepkg --clean {{path/to/directory}}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
