{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605d132a-e3b2-42ee-82a2-50ccefbc9139",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf93b4f-f9b0-4929-bd2c-4bb7688d8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%pip install ollama\n",
    "%pip install openai\n",
    "%pip install transformers\n",
    "%pip install tiktoken\n",
    "%pip install huggingface-hub\n",
    "%pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a3c3c-35ac-4f7f-836b-ed72851c222e",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50df78-1de1-470c-b5aa-10c69228ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.append('../')  # Add the path to the my_packages module\n",
    "from my_packages.data_processing.split_dataset import split_on_shots, split\n",
    "from my_packages.data_processing.get_labels_data import used_libraries_from_dataset\n",
    "from my_packages.analysis.analyze_datasets import analyze_library_distribution, analyze_instance_distribution, analyze_visual_node_types_distribution\n",
    "\n",
    "def used_libraries_to_string(data):\n",
    "    name_doc_string = \"\"\n",
    "    for func in data:\n",
    "        name_doc_string += f\"Name: {func['function_name']}\\nDocumentation: {func['doc']}\\n\\n\"\n",
    "    return name_doc_string\n",
    "\n",
    "main_dataset_folder = '../data/mbpp_transformed_code_examples/sanitized-MBPP-midio.json'\n",
    "\n",
    "# main dataset\n",
    "with open(main_dataset_folder, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "    \n",
    "num_shot = 10 # Few-shot examples\n",
    "eval_size_percentage = 0.5\n",
    "train_data, val_data, test_data = split_on_shots(num_shot, eval_size_percentage, dataset, seed = 64, write_to_file=True)\n",
    "\n",
    "# Extract all unique nodes (library_functions) across datasets\n",
    "used_libraries_json = used_libraries_from_dataset(train_data)\n",
    "explained_used_libraries = used_libraries_to_string(used_libraries_json)\n",
    "\n",
    "#Bar chart of distribuation\n",
    "analyze_library_distribution(train_data, val_data, test_data)\n",
    "analyze_instance_distribution(train_data, val_data, test_data)\n",
    "analyze_visual_node_types_distribution(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ed589",
   "metadata": {},
   "source": [
    "# Prompt utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d51ed17d-7b3a-469b-8da6-bfb763f9e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "def extract_code(response_text):\n",
    "    \"\"\"Extract code snippet from the response using regex.\"\"\"\n",
    "    # Match content between ```language and ```\n",
    "    match = re.search(fr\"```midio(.*?)```\", response_text, re.DOTALL)\n",
    "\n",
    "    # Extract and clean up the code\n",
    "    if match:\n",
    "        return match.group(1).strip()  # Return only the code block\n",
    "\n",
    "    # If no match, assume the response might already be code without markdown formatting\n",
    "    return response_text.strip()\n",
    "\n",
    "def read_file(_file):\n",
    "    with open(_file) as reader:\n",
    "        return reader.read()\n",
    "\n",
    "def get_prompt_responses(data: json, code_folder : str):\n",
    "    script_path = os.path.dirname(os.getcwd())\n",
    "    prompt_template_path = os.path.join(script_path, 'templates/prompts/FEW_SHOT_TEMPLATE.file')\n",
    "    code_template_path = os.path.join(script_path, 'templates/responses/CODE_TEMPLATE.file')\n",
    "\n",
    "    if not (os.path.exists(prompt_template_path)):\n",
    "        print(\"Few_shot_template.file not found!!\")\n",
    "        return\n",
    "    if not (os.path.exists(code_template_path)):\n",
    "        print(\"Code_template.file not found!!\")\n",
    "        return\n",
    "    prompt_template = read_file(prompt_template_path)\n",
    "    code_template = read_file(code_template_path)\n",
    "\n",
    "    prompts = [prompt_template.format(task_description=task['prompts'][0]) for task in data]\n",
    "    responses = []\n",
    "    for task in data:\n",
    "        file_path = f\"{code_folder}/task_id_{task['task_id']}.midio\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                solution_code = code_template.format(code=file.read().strip())\n",
    "                responses.append(solution_code)\n",
    "        except FileNotFoundError:\n",
    "            responses.append(\"File not found\")\n",
    "        except Exception as e:\n",
    "            responses.append(f\"Error: {e}\")\n",
    "    return (prompts, responses)\n",
    "\n",
    "def create_few_shot_prompt(train_prompts, train_responses, context_role= \"developer\"):\n",
    "    guiding = \"You are a Midio code generator and are going to solve some programming tasks for a node-based programming language called Midio. Always format output strictly inside code blocks. Generate only the code and no text, so that it can be directly executed in a compiler. Use minimal amount of library functions to solve the tasks.\\n\" \n",
    "    node_list = f\"Only use the following library functions:\\n {explained_used_libraries}\\n\\n\"\n",
    "    context = guiding + node_list\n",
    "    few_shots_messages = [{\"role\": context_role, \"content\": context}] \n",
    "    for i, (prompt, response) in enumerate(zip(train_prompts, train_responses)):\n",
    "        few_shots_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        few_shots_messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return few_shots_messages\n",
    "\n",
    "train_prompts, train_responses = get_prompt_responses(train_data, '../data/mbpp_transformed_code_examples/only_files/')\n",
    "val_prompts, val_responses = get_prompt_responses(val_data, '../data/mbpp_transformed_code_examples/only_files/')\n",
    "test_prompts, test_responses = get_prompt_responses(test_data, '../data/mbpp_transformed_code_examples/only_files/')\n",
    "\n",
    "few_shot_messages = create_few_shot_prompt(train_prompts, train_responses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3435a07d",
   "metadata": {},
   "source": [
    "# Open source models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b7b89",
   "metadata": {},
   "source": [
    "Use OpenAI python library, but connect it to ollama to use open source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from openai import OpenAI\n",
    "host = 'http://localhost:11434'\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=host+'/v1/',\n",
    "  api_key='ollama' #Ignored with 'ollama'. To use GPT models, this key need to be set.\n",
    ")\n",
    "\n",
    "# To be able to pull (download) models from ollama in the code, ollama.pull('model_name'). Not possible with the openAI client.\n",
    "ollama.Client(\n",
    "  host=host,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42dd3f",
   "metadata": {},
   "source": [
    "## Models to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b080bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# # Log in to Hugging Face\n",
    "# load_dotenv(\"../.env\")\n",
    "# access_token_read = os.environ.get('HF_API_KEY')\n",
    "# if access_token_read:\n",
    "#     login(token=access_token_read)\n",
    "#     print(\"Logged in to Hugging Face successfully!\")\n",
    "# else:\n",
    "#     print(\"HF_API_KEY is not set in your environment variables.\")\n",
    "\n",
    "open_models = [\n",
    "    #llama models:\n",
    "    {\"name\": \"llama3.1:8b-instruct-fp16\"},\n",
    "    {\"name\": \"llama3.3:70b-instruct-q8_0\"},\n",
    "    # {\"name\": \"llama3.3:70b-instruct-fp16\"},\n",
    "\n",
    "    {\"name\": \"mistral-small:22b-instruct-2409-fp16\"},\n",
    "    {\"name\": \"codestral:22b-v0.1-q8_0\"},\n",
    "    #Not downloaded bellow\n",
    "    {\"name\": \"mistral-large:123b-instruct-2407-q4_K_M\"},\n",
    "    \n",
    "    {\"name\": \"qwq:32b-preview-fp16\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d77b4",
   "metadata": {},
   "source": [
    "## Calculate max tokens for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from my_packages.utils.tokens import find_max_tokens_code_tokenizer, find_max_tokens_code_api\n",
    "\n",
    "DATA_DIR = '../data/mbpp_transformed_code_examples/only_files'\n",
    "\n",
    "open_models_to_test = []\n",
    "for model_info in open_models:\n",
    "    # ollama.pull(model_info[\"name\"])\n",
    "    max_tokens = find_max_tokens_code_api(DATA_DIR, model_info[\"name\"], client.embeddings)\n",
    "    model_result = {\n",
    "        \"name\": model_info[\"name\"],\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    open_models_to_test.append(model_result)\n",
    "\n",
    "print(open_models_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca2e76",
   "metadata": {},
   "source": [
    "# GPT-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c916afd",
   "metadata": {},
   "source": [
    "## Models to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1eeeb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_models = [\n",
    "    {\"name\": \"o1-preview\", \"tokenization\": \"o200k_base\"},\n",
    "    {\"name\": \"gpt-4o\", \"tokenization\": \"o200k_base\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413bfa6f",
   "metadata": {},
   "source": [
    "## Calculate max tokens for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "gpt_models_to_test = []\n",
    "for model_info in gpt_models:\n",
    "    encoding = tiktoken.encoding_for_model(model_info[\"name\"])\n",
    "    max_tokens = find_max_tokens_code_tokenizer(DATA_DIR, encoding)\n",
    "    model_result = {\n",
    "        \"name\": model_info[\"name\"],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    gpt_models_to_test.append(model_result)\n",
    "print(gpt_models_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d05ea3-4062-48f7-94c9-51c0cfb96505",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d37bc4",
   "metadata": {},
   "source": [
    "## Functions for evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fd4f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_packages.evaluation.midio_compiler import compile_code, is_code_syntax_valid, is_code_semantically_valid, print_compiled_output\n",
    "# from my_packages.utils.run_bash_script import run_bash_script_with_ssh\n",
    "from my_packages.utils.ollama_utils import is_remote_server_reachable\n",
    "import time\n",
    "from subprocess import call\n",
    "\n",
    "\n",
    "# Function to generate and evaluate responses\n",
    "def evaluate(client, model, prompts, responses, seed, max_new_tokens=50, temperature=0.7, top_p=0.9):\n",
    "    correct_syntax = 0 # Number of samples that is syntactically correct\n",
    "    correct_semantic = 0 # Number of samples that is both syntactically and semantically correct\n",
    "    # all_tests_passed = 0 # Number of samples that passed all tests\n",
    "    total = len(prompts)\n",
    "    # if model.startswith(\"o1\"): #o1 models are not working with developer or system role\n",
    "    #     print(\"Testing GPT model\")\n",
    "    #     few_shot_messages = create_few_shot_prompt(train_prompts, train_responses, context_role=\"user\")\n",
    "    # else:\n",
    "    #     few_shot_messages = create_few_shot_prompt(train_prompts, train_responses, context_role=\"developer\")\n",
    "\n",
    "    for index, (prompt, true_response) in enumerate(zip(prompts, responses)):\n",
    "        max_retries = 3\n",
    "        retries = 0\n",
    "\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                print(f\"Generating response..\", end=\"\\r\")\n",
    "                generated = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=few_shot_messages + [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    # template=\"{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\\n\\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\\n\\n```midio\\n{{ .Response }}\\n```<|eot_id|>\", # Template enforces code block\n",
    "                    max_tokens=max_new_tokens,\n",
    "                    seed=seed,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    stream=False,\n",
    "                    stop=[\"```<|eot_id|>\"]  # Ensure the response stops after the code block\n",
    "                )\n",
    "                filtered_generated = generated.choices[0].message.content.replace(\"//\", \"\").strip() # '//' outside main module can lead to compiler not ending properly\n",
    "                generated_code = extract_code(filtered_generated)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Attempt {retries} failed with error: {e}\")\n",
    "                if is_remote_server_reachable(host + \"/api/tags\"):\n",
    "                    print(\"Server is reachable.\")\n",
    "                else:\n",
    "                    rc = call(\"../SSH_FORWARDING.sh\")\n",
    "                    # Check the result\n",
    "                    if rc == 0:\n",
    "                        print(\"Command executed successfully!\")\n",
    "                    else:\n",
    "                        print(f\"SSH command failed with return code {rc}\")\n",
    "                        print(f\"Try to manually connect to the server again. \")\n",
    "                        #Wait one minute before trying again\n",
    "                        try:\n",
    "                            for remaining in range(60, 0, -1):\n",
    "                                print(f\"Next try in {remaining} seconds\", end=\"\\r\")  # Print on the same line\n",
    "                                time.sleep(1)  # Wait for 1 second\n",
    "                            print(\"Time's up!                          \\n\")  # Clear the line after completion\n",
    "                        except KeyboardInterrupt:\n",
    "                            print(\"\\nCountdown interrupted!\")\n",
    "    \n",
    "        else:\n",
    "            print(\"Failed to get a response from the server after \" + str(retries) + \" attempts.\")\n",
    "            generated_code = \"\"\n",
    "            return -1\n",
    "\n",
    "        # print(f\"\\n\\nSample: {index}\")\n",
    "        # print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated response:\\n {filtered_generated}\")\n",
    "        # print(f\"True response:\\n {true_response}\")\n",
    "        compiled = compile_code(generated_code)\n",
    "        # print_compiled_output(compiled)\n",
    "        if is_code_syntax_valid(compiled):\n",
    "            correct_syntax += 1\n",
    "            print(\"Code parsed successfully!\")\n",
    "            if is_code_semantically_valid(compiled):\n",
    "                correct_semantic += 1\n",
    "                print(\"Code is semantically valid!\")\n",
    "            # Does not mean it semantically correct, but at least it compiles\n",
    "            # Errors on semantics are not checked here, but logged in the is_code_compilable function\n",
    "        else:\n",
    "            print(\"Compilation failed.\\n\")\n",
    "\n",
    "    return (correct_syntax / total, correct_semantic / total)\n",
    "\n",
    "def evaluate_models(client, models_to_test, temperatures=[0.5, 0.7, 0.9], top_ps=[0.2, 0.5, 1.0], seeds=[3, 75, 346]):\n",
    "    results = {}\n",
    "\n",
    "    for model in models_to_test:\n",
    "        print(f\"Model: {model['name']}\")\n",
    "\n",
    "        SEED = 42 # During Validation Phase for reproducibility\n",
    "\n",
    "        best_syntax_accuracy = 0\n",
    "        best_semantic_accuracy = 0\n",
    "        best_params = {\"temperature\": 0.7, \"top_p\": 0.9}\n",
    "\n",
    "        for temp in temperatures:\n",
    "            for top_p in top_ps:\n",
    "                syntax_accuracy, semantic_accuracy = evaluate (\n",
    "                    client,\n",
    "                    model['name'],\n",
    "                    val_prompts,\n",
    "                    val_responses,\n",
    "                    SEED,\n",
    "                    model[\"max_tokens\"],\n",
    "                    temp,\n",
    "                    top_p\n",
    "                )\n",
    "                # print(f\"Tested with temp={temp} and top_p={top_p}. Gave syntax_accuracy={syntax_accuracy}\")\n",
    "                if syntax_accuracy > best_syntax_accuracy:\n",
    "                    best_syntax_accuracy = syntax_accuracy\n",
    "                if semantic_accuracy > best_semantic_accuracy:\n",
    "                    best_semantic_accuracy = semantic_accuracy\n",
    "                    best_params = {\"temperature\": temp, \"top_p\": top_p}\n",
    "                \n",
    "        print(f\"Best Hyperparameters for {model['name']}: {best_params}, Validation Syntax Accuracy: {best_syntax_accuracy:.2f}, Validation Semantic Accuracy: {best_semantic_accuracy:.2f}\")\n",
    "    \n",
    "        #Test the model on the test setm with different seeds and the best hyperparameters.\n",
    "        for seed in seeds:\n",
    "            print(f\"\\nTesting with Seed: {seed}\")\n",
    "\n",
    "            test_syntax_accuracy, test_semantic_accuracy = evaluate(\n",
    "                client,\n",
    "                model['name'],\n",
    "                test_prompts,\n",
    "                test_responses,\n",
    "                seed,\n",
    "                model[\"max_tokens\"],\n",
    "                temperature=best_params[\"temperature\"],\n",
    "                top_p=best_params[\"top_p\"]\n",
    "            )\n",
    "\n",
    "            print(f\"Test Syntax Accuracy for {model['name']}: {test_syntax_accuracy:.2f}\")\n",
    "            print(f\"Test Semantic Accuracy for {model['name']}: {test_semantic_accuracy:.2f}\")\n",
    "\n",
    "            if model[\"name\"] not in results:\n",
    "                results[model[\"name\"]] = []\n",
    "            \n",
    "            results[model[\"name\"]].append({\n",
    "                \"seed\": seed,\n",
    "                \"validation_syntax_accuracy\": best_syntax_accuracy,\n",
    "                \"validation_semantic_accuracy\": best_semantic_accuracy,\n",
    "                \"test_syntax_accuracy\": test_syntax_accuracy,\n",
    "                \"test_semantic_accuracy\": test_semantic_accuracy\n",
    "            })\n",
    "\n",
    "    # Print the final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for model_name, runs in results.items():\n",
    "        print(f\"Model: {model_name}\")\n",
    "        for run in runs:\n",
    "            val_syn = run[\"validation_syntax_accuracy\"]\n",
    "            val_sem = run[\"validation_semantic_accuracy\"]\n",
    "            test_syn = run[\"test_syntax_accuracy\"]\n",
    "            test_sem = run[\"test_semantic_accuracy\"]\n",
    "            print(\n",
    "                f\"  Seed {run['seed']}: \"\n",
    "                f\"Val Syntax Acc: {val_syn:.2f}, \"\n",
    "                f\"Val Semantic Acc: {val_sem:.2f}, \"\n",
    "                f\"Test Syntax Acc: {test_syn:.2f}, \"\n",
    "                f\"Test Semantic Acc: {test_sem:.2f}\"\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2173520f",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2fa24",
   "metadata": {},
   "source": [
    "### Open source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40466c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_packages.utils.ollama_utils import is_remote_server_reachable\n",
    "from openai import OpenAI\n",
    "\n",
    "host = 'http://localhost:11434'\n",
    "\n",
    "if is_remote_server_reachable(host + \"/api/tags\"):\n",
    "    print(\"Server is reachable.\")\n",
    "else:\n",
    "    rc = call(\"../SSH_FORWARDING.sh\")\n",
    "    print(\"Ollama server is not reachable. Batch job might have finished. Try running bash script again.\")\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=host+'/v1/',\n",
    "  api_key='ollama' #Ignored with 'ollama'. To use GPT models, this key need to be set.\n",
    ")\n",
    "evaluate_models(client, open_models_to_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435b2f3",
   "metadata": {},
   "source": [
    "### GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b5510-e0d2-42ca-825e-0a43eb7e3d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")\n",
    "# Get the OpenAI API key from environment variables\n",
    "openai_token = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_token:\n",
    "    client = OpenAI(\n",
    "        api_key=openai_token  # Use the OpenAI API key\n",
    "    )\n",
    "    evaluate_models(client, gpt_models_to_test)\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is not set in your environment variables.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
