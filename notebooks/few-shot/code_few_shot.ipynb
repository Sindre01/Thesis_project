{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605d132a-e3b2-42ee-82a2-50ccefbc9139",
   "metadata": {},
   "source": [
    "<!-- ## Install dependencies -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf93b4f-f9b0-4929-bd2c-4bb7688d8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For Notebook:\n",
    "# %pip install ollama\n",
    "# %pip install openai\n",
    "# %pip install anthropic\n",
    "# %pip install transformers\n",
    "# %pip install tiktoken\n",
    "# %pip install python-dotenv\n",
    "# %pip install colorama\n",
    "\n",
    "\n",
    "\n",
    "# For packages: \n",
    "# %pip install scikit-learn\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install iterative-stratification\n",
    "# %pip install langchain\n",
    "# %pip install \"langsmith<0.2.0\"\n",
    "\n",
    "\n",
    "# For WebUI testing:\n",
    "# %pip install open-webui\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b4428-6b24-441c-90da-44e03336f79d",
   "metadata": {},
   "source": [
    "<!-- # Get data from Langsmith -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3fe237",
   "metadata": {},
   "source": [
    "# Get dataset from Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51ed17d-7b3a-469b-8da6-bfb763f9e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing dataset found.\n",
      "dataset_id=UUID('9ddd654d-81b4-4cce-a220-3b064c44b844') inputs={'task': 'Create a function to check whether the given number is a perfect square or not.'} outputs={'response': 'import(\"std\", Std_k98ojb)\\nimport(\"http\", Http_q7o96c)\\n\\nmodule() main {\\n    func(doc: \"checks whether the given number is a perfect square or not.\") is_perfect_square {\\n        in(x: 354, y: -53, name: \"input\") property(Number) input_f5c03d\\n        in(x: 161, y: -211, name: \"execute\") trigger() execute_27b8c3\\n\\n        out(x: 1748, y: -10, name: \"out\") property(Bool) out_f3db9f\\n        out(x: 1894, y: 120, name: \"continue\") trigger() continue_8ba06b\\n\\n        instance(x: 352, y: -493) map_6030c9 root.Std_k98ojb.Iteration.Map {}\\n        instance(x: 591, y: 74) generaterange_038fce root.Std_k98ojb.List.GenerateRange {}\\n        instance(x: 933, y: 25) lessthanorequal_f11fda root.Std_k98ojb.Math.LessThanOrEqual {}\\n        instance(x: 714, y: -155) for_8cb3f6 root.Std_k98ojb.Std.For {}\\n        instance(x: 1812, y: -319) and_80a119 root.Std_k98ojb.Logic.And {}\\n        instance(x: 1121, y: -54) if_07b8c7 root.Std_k98ojb.Std.If {}\\n        instance(x: 1504, y: -400) expression_b720d7 root.Std_k98ojb.Math.Expression {\\n            expression: \"(n % i == 0)\"\\n        }\\n        instance(x: 1496, y: -187) expression_9fba8c root.Std_k98ojb.Math.Expression {\\n            expression: \"(n / i == i)\"\\n        }\\n        instance(x: 2136, y: -356) if_e19a62 root.Std_k98ojb.Std.If {}\\n        instance(x: 1088, y: -256) add_b4554f root.Std_k98ojb.Math.Add {}\\n        1 -> generaterange_038fce.from\\n        input_f5c03d -> generaterange_038fce.to\\n        generaterange_038fce.list -> map_6030c9.items\\n        func() {\\n            in(x: -76, y: 46, name: \"element\") property(Number) element_5f51f7\\n\\n            out(x: 382, y: 19, name: \"out\") property(Number) out_d0fd24\\n\\n            instance(x: 106, y: 17) mul_2f9d09 root.Std_k98ojb.Math.Mul {}\\n            element_5f51f7 -> mul_2f9d09.first\\n            element_5f51f7 -> mul_2f9d09.second\\n            mul_2f9d09.result -> out_d0fd24\\n        } -> map_6030c9.handler\\n        map_6030c9.output_list -> for_8cb3f6.items\\n        for_8cb3f6.item -> lessthanorequal_f11fda.left\\n        input_f5c03d -> lessthanorequal_f11fda.right\\n        lessthanorequal_f11fda.result -> if_07b8c7.predicate\\n        for_8cb3f6.onItem -> if_07b8c7.execute\\n        if_07b8c7.else -> continue_8ba06b\\n        expression_b720d7.result -> and_80a119.left\\n        expression_9fba8c.result -> and_80a119.right\\n        and_80a119.result -> if_e19a62.predicate\\n        if_07b8c7.then -> if_e19a62.execute\\n        for_8cb3f6.index -> add_b4554f.first\\n        1 -> add_b4554f.second\\n        add_b4554f.result -> expression_b720d7.gen_1\\n        add_b4554f.result -> expression_9fba8c.gen_1\\n        input_f5c03d -> expression_9fba8c.gen_0\\n        input_f5c03d -> expression_b720d7.gen_0\\n        if_07b8c7.value -> out_f3db9f\\n        if_e19a62.then -> continue_8ba06b\\n        for_8cb3f6.done -> continue_8ba06b\\n        execute_27b8c3 -> for_8cb3f6.reset\\n        if_e19a62.else -> for_8cb3f6.next\\n    }\\n\\n    instance(x: -189, y: 222) is_perfect_square_f0be66 root.main.is_perfect_square {}\\n}'} metadata={'task_id': '50', 'testing': {'tests': {'test_cases': [{'input': 10, 'expected_output': False}, {'input': 36, 'expected_output': True}, {'input': 14, 'expected_output': False}]}, 'function_signature': 'func(doc: \"checks whether the given number is a perfect square or not.\") is_perfect_square {\\nin(x: 354.8534228128, y: -53.570902536908875, name: \"input\") property(Number) input_f5c03d\\n in(x: 161.86656944950778, y: -211.44732045205976, name: \"execute\") trigger() execute_27b8c3\\n out(x: 1748.7398492866953, y: -10.179326168336845, name: \"out\") property(Bool) out_f3db9f\\n out(x: 1894.6987078855211, y: 120.75836251660428, name: \"continue\") trigger() continue_8ba06b'}, 'MBPP_task_id': '803', 'dataset_split': ['train'], 'external_functions': 'Iteration.Map, List.GenerateRange, Math.LessThanOrEqual, Std.For, Logic.And, Std.If, Math.Expression, Math.Add, Math.Mul'} id=UUID('d5cfaa41-ff3f-4cba-9a12-4ef6b820db98') created_at=datetime.datetime(2025, 2, 12, 9, 2, 16, 578345, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2025, 2, 12, 9, 2, 16, 578345, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None attachments={}\n",
      "Number of train examples loaded: 32\n",
      "Number of validation examples loaded: 9\n",
      "Number of test examples loaded: 9\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "from langsmith.schemas import Example\n",
    "\n",
    "dataset_name = \"Code prediction\"\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "langsmith_client = Client()\n",
    "\n",
    "if langsmith_client.has_dataset(dataset_name=dataset_name):\n",
    "    langsmith_dataset=langsmith_client.read_dataset(dataset_name=dataset_name)\n",
    "    print(\"Existing dataset found.\")\n",
    "    \n",
    "    train_data = list(langsmith_client.list_examples(\n",
    "        dataset_name=dataset_name,\n",
    "        splits=[\"train\"],\n",
    "        # metadata={\"task_id\": \"1\"},\n",
    "        # limit=1\n",
    "    ))\n",
    "    \n",
    "\n",
    "    # train_data = [example.dict(include={\"inputs\", \"outputs\", \"metadata\"}) for example in train_data]\n",
    "    print(train_data[0])\n",
    "    print(f\"Number of train examples loaded: {len(train_data)}\")\n",
    "    \n",
    "    val_data = list(langsmith_client.list_examples(\n",
    "        dataset_name=dataset_name,\n",
    "        splits=[\"validation\"],\n",
    "        # metadata={\"task_id\": \"1\"},\n",
    "        # limit=5\n",
    "    ))\n",
    "    print(f\"Number of validation examples loaded: {len(val_data)}\")\n",
    "    # val_data = [example.dict(include={\"inputs\", \"outputs\", \"metadata\"}) for example in val_data]\n",
    "\n",
    "\n",
    "    test_data = list(langsmith_client.list_examples(\n",
    "        dataset_name=dataset_name,\n",
    "        splits=[\"test\"],\n",
    "        # metadata={\"task_id\": \"1\"},\n",
    "        # limit=5\n",
    "    ))\n",
    "    print(f\"Number of test examples loaded: {len(test_data)}\")\n",
    "    # test_data = [example.dict(include={\"inputs\", \"outputs\", \"metadata\"}) for example in test_data]\n",
    "    \n",
    "    # make the rest of the code work with Example object, instead of [{'task_id': str, 'task': str, 'response': list, 'MBPP_task_id': str}]. \n",
    "    # Create an database of type chat\n",
    " \n",
    "else:\n",
    "    print(f\"No existing dataset found with name: {dataset_name}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cf60e",
   "metadata": {},
   "source": [
    "<!-- # Init model provider -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeea479",
   "metadata": {},
   "source": [
    "# Model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5330f865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all responses: 50\n",
      "Server is reachable.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "sys.path.append('../../')  # Add the path to the my_packages module\n",
    "from my_packages.utils.tokens_utils import models_not_in_file, write_models_tokens_to_file\n",
    "from my_packages.utils.server_utils import server_diagnostics, is_remote_server_reachable\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "model_provider = 'ollama'\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "all_responses = [sample.outputs['response'] for sample in train_data] + [sample.outputs['response'] for sample in val_data] + [sample.outputs['response'] for sample in test_data]\n",
    "print(f\"Number of all responses: {len(all_responses)}\")\n",
    "\n",
    "match model_provider:\n",
    "    case 'ollama':\n",
    "        host = 'http://localhost:11434'\n",
    "        if is_remote_server_reachable(url = host + \"/api/tags\"):\n",
    "            print(\"Server is reachable.\")\n",
    "        else:\n",
    "            server_diagnostics()\n",
    "            print(\"Ollama server is not reachable. Batch job might have finished. Try running bash script again.\")\n",
    "        client = ChatOllama\n",
    "   \n",
    "    \n",
    "        models = [\n",
    "            # 14b models:\n",
    "            # \"phi4:14b-fp16\", #16k context length\n",
    "            \"deepseek-r1:14b\",\n",
    "            \"qwen2.5:14b-instruct-fp16\", #128 k\n",
    "\n",
    "            #32b models:\n",
    "            \"qwq:32b-preview-fp16\",\n",
    "            \"qwen2.5-coder:32b-instruct-fp16\", #32,768 tokens\n",
    " \n",
    "            #70b models:\n",
    "            \"llama3.3:70b-instruct-fp16\",\n",
    "            \"qwen2.5:72b-instruct-fp16\",\n",
    "        ]\n",
    "        models_not_tokenized = models_not_in_file(models, 'code_max_tokens.json')\n",
    "        write_models_tokens_to_file(client, models_not_tokenized, all_responses, 'code_max_tokens.json')\n",
    "\n",
    "    case 'openai':\n",
    "        openai_token = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_token:\n",
    "            raise Exception(\"OpenAI API key not found in .env file\")\n",
    "        client = ChatOpenAI\n",
    "        models = [\n",
    "            \"gpt-4o\",\n",
    "            # \"o1-preview\", \n",
    "        ]\n",
    "        # few_shot_messages = create_few_shot_messages(explained_used_libraries, train_prompts, train_responses, \"NODE_GENERATOR_TEMPLATE\", \"developer\")\n",
    "        models_not_tokenized = models_not_in_file(models, 'code_max_tokens.json')\n",
    "        write_models_tokens_to_file(client, models_not_tokenized, all_responses, 'code_max_tokens.json')\n",
    "\n",
    "    case 'anthropic':\n",
    "        anthropic_token = os.getenv('ANTHROPIC_API_KEY')\n",
    "        if not anthropic_token:\n",
    "            raise Exception(\"Anthropic API key not found in .env file\")\n",
    "        client = ChatAnthropic\n",
    "        # embed_client = AnthropicEmbeddings\n",
    "        models = [\n",
    "            \"claude-3-5-sonnet-latest\"\n",
    "        ]\n",
    "        models_not_tokenized = models_not_in_file(models, 'code_max_tokens.json')\n",
    "        write_models_tokens_to_file(client, models_not_tokenized, all_responses, 'code_max_tokens.json')\n",
    "    case _:\n",
    "        raise Exception(\"Model provider not supported\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b075a40",
   "metadata": {},
   "source": [
    "# Task configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a6d5c",
   "metadata": {},
   "source": [
    "<!-- ## Experiments settings -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c44196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library functions included in the dataset: 51\n"
     ]
    }
   ],
   "source": [
    "from my_packages.data_processing.attributes_processing import used_functions_from_dataset, used_functions_to_string\n",
    "from my_packages.data_processing.split_dataset import read_dataset_to_json\n",
    "\n",
    "prompt_prefix = \"Create a function\" # \"e.g., \"Create a flow\"\n",
    "NUM_SHOTS = 5\n",
    "semantic_selector = False\n",
    "\n",
    "main_dataset_folder = '../../data/MBPP-Midio-50.json'\n",
    "dataset = read_dataset_to_json(main_dataset_folder)\n",
    "used_functions_json = used_functions_from_dataset(dataset)\n",
    "available_nodes = used_functions_to_string(used_functions_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a530b",
   "metadata": {},
   "source": [
    "# Init Example selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9f228b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in the pool: 32\n",
      "Computes selection\n",
      "50\n",
      "29\n",
      "14\n",
      "47\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../../')  # Add the path to the my_packages module\n",
    "from my_packages.prompting.example_selectors import get_coverage_example_selector, get_semantic_similarity_example_selector\n",
    "\n",
    "def example_to_dict(example):\n",
    "    example = example.dict()\n",
    "    return {\n",
    "        \"task\": example[\"inputs\"][\"task\"],\n",
    "        \"response\": example[\"outputs\"][\"response\"],\n",
    "        \"task_id\": example[\"metadata\"][\"task_id\"],\n",
    "        \"MBPP_task_id\": example[\"metadata\"][\"MBPP_task_id\"],\n",
    "        \"external_functions\": example[\"metadata\"][\"external_functions\"]\n",
    "    }\n",
    "\n",
    "# Transform train_data to a list of dictionaries, and sort them by task_id\n",
    "example_pool = [example_to_dict(example) for example in train_data]\n",
    "example_pool.sort(key=lambda x: int(x['task_id']))\n",
    "print(f\"Number of examples in the pool: {len(example_pool)}\")\n",
    "\n",
    "if semantic_selector:\n",
    "    selector = get_semantic_similarity_example_selector(\n",
    "        example_pool, \n",
    "        OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "        shots=NUM_SHOTS,\n",
    "        input_keys=[\"task\"],\n",
    "    )\n",
    "else:\n",
    "    selector = get_coverage_example_selector(\n",
    "        example_pool, \n",
    "        label = \"external_functions\",\n",
    "        shots=NUM_SHOTS,\n",
    "        seed=9\n",
    "    )\n",
    "\n",
    "examples = selector.select_examples({\"task\": \"Create a list of all the unique elements in a list.\"})\n",
    "\n",
    "for e in examples:\n",
    "    print(e['task_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d30fcb4",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3807a2",
   "metadata": {},
   "source": [
    "#### Testing functional correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e242cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../../')  # Add the path to the my_packages module\n",
    "# from my_packages.evaluation.code_evaluation import calculate_pass_at_k_scores\n",
    "# from my_packages.evaluation.metrics import check_correctness\n",
    "# all_data = train_data + val_data + test_data\n",
    "# results = {}\n",
    "# for example in all_data:\n",
    "#     task_id = example.metadata['task_id']\n",
    "#     # if task_id in results:\n",
    "#     #     continue\n",
    "#     results[task_id] = [example.outputs['response']]\n",
    "\n",
    "# test_results = check_correctness(results)\n",
    "# print(\"\\n\\nTESTS FAILED:\")\n",
    "# for task_id, responses in test_results.items():\n",
    "#     if not responses[0]['passed']:\n",
    "#         print(task_id)\n",
    "#         print(responses[0])\n",
    "# pass_at_ks = calculate_pass_at_k_scores(results, ks=[1], metric=\"tests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce8f017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deepseek-r1:14b\n",
      "\n",
      "===  VALIDATION ===\n",
      "\u001b[32m\u001b[1m  > Temperature: 0.60\n",
      "  > Top_k: 10.00\n",
      "  > Top_p: 0.90\n",
      "\u001b[0m  > Optimized metric result:: {\n",
      "    \"pass@1\": 0.2\n",
      "} > Metadata: null\n",
      "\u001b[36m\u001b[1mTesting Phase:\u001b[0m\n",
      "\n",
      "Uses signature prompt!.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'func_signature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     45\u001b[0m         validation_result \u001b[38;5;241m=\u001b[39m run_validation(\n\u001b[1;32m     46\u001b[0m             client,\n\u001b[1;32m     47\u001b[0m             model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m             optimizer_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# \"semantic\"  \"syntax\", \"tests\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[1;32m     59\u001b[0m     validation_result\u001b[38;5;241m.\u001b[39mprint()\n\u001b[0;32m---> 61\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_testing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mavailable_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# k generations per task\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m346\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtests\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# current metrics: \"semantic\",  \"syntax\", \"tests\"\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     results[model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m (validation_result, test_results)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Print the final results\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis_project/notebooks/few-shot/../../my_packages/evaluation/code_evaluation.py:397\u001b[0m, in \u001b[0;36mrun_testing\u001b[0;34m(client, model, available_nodes, test_data, example_pool, temperature, top_p, top_k, ks, seeds, debug, metrics)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting with Seed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ..\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 397\u001b[0m     metric_results_lists, largest_context \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mavailable_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     new_run \u001b[38;5;241m=\u001b[39m Run(\n\u001b[1;32m    414\u001b[0m         phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    415\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlargest_prompt_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: largest_context}\n\u001b[1;32m    426\u001b[0m     )\n\u001b[1;32m    428\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(new_run)\n",
      "File \u001b[0;32m~/Thesis_project/notebooks/few-shot/../../my_packages/evaluation/code_evaluation.py:287\u001b[0m, in \u001b[0;36mevaluate_code\u001b[0;34m(client, model, available_nodes, data, example_pool, max_new_tokens, temperature, top_p, top_k, seed, ks, debug, evaluation_metric)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_code\u001b[39m(\n\u001b[1;32m    272\u001b[0m     client: ChatOllama \u001b[38;5;241m|\u001b[39m ChatOpenAI \u001b[38;5;241m|\u001b[39m ChatAnthropic,\n\u001b[1;32m    273\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     evaluation_metric \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msyntax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtests\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    285\u001b[0m )\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]], \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 287\u001b[0m     model_result, largest_context \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mavailable_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation_metric\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     metric_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m evaluation_metric:\n",
      "File \u001b[0;32m~/Thesis_project/notebooks/few-shot/../../my_packages/evaluation/code_evaluation.py:183\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(client, model, available_nodes, data, example_pool, max_new_tokens, temperature, top_p, top_k, ks, seed, debug, metrics)\u001b[0m\n\u001b[1;32m    181\u001b[0m     final_prompt_template \u001b[38;5;241m=\u001b[39m create_final_prompt(few_shot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCODE_GENERATOR_TEMPLATE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCODE_SIGNATURE_TEMPLATE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    182\u001b[0m     function_signature \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_signature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 183\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_prompt_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_signature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexternal_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavailable_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[1;32m    185\u001b[0m     few_shot \u001b[38;5;241m=\u001b[39m create_few_shot_prompt(examples, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCODE_TEMPLATE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Thesis_project/.venv_Master/lib/python3.11/site-packages/langchain_core/prompts/chat.py:760\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    751\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format the chat template into a string.\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \n\u001b[1;32m    753\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03m        formatted string.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_string()\n",
      "File \u001b[0;32m~/Thesis_project/.venv_Master/lib/python3.11/site-packages/langchain_core/prompts/chat.py:783\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m    775\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;124;03m        PromptValue.\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 783\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[0;32m~/Thesis_project/.venv_Master/lib/python3.11/site-packages/langchain_core/prompts/chat.py:1224\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1222\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[1;32m   1223\u001b[0m ):\n\u001b[0;32m-> 1224\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mmessage_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1225\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Thesis_project/.venv_Master/lib/python3.11/site-packages/langchain_core/prompts/few_shot.py:390\u001b[0m, in \u001b[0;36mFewShotChatMessagePromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    395\u001b[0m     message\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample)\n\u001b[1;32m    398\u001b[0m ]\n",
      "File \u001b[0;32m~/Thesis_project/.venv_Master/lib/python3.11/site-packages/langchain_core/prompts/few_shot.py:391\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    390\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 391\u001b[0m     \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m\u001b[43m}\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[1;32m    392\u001b[0m ]\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    395\u001b[0m     message\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample)\n\u001b[1;32m    398\u001b[0m ]\n",
      "File \u001b[0;32m~/Thesis_project/.venv_Master/lib/python3.11/site-packages/langchain_core/prompts/few_shot.py:391\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    390\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 391\u001b[0m     {k: \u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39minput_variables} \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[1;32m    392\u001b[0m ]\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    395\u001b[0m     message\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample)\n\u001b[1;32m    398\u001b[0m ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'func_signature'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.append('../../')  # Add the path to the my_packages module\n",
    "from my_packages.evaluation.code_evaluation import Run, run_validation, run_testing, calculate_final_result\n",
    "from my_packages.utils.tokens_utils import get_model_code_tokens_from_file\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "def example_to_dict(example):\n",
    "    example = example.dict()\n",
    "    return {\n",
    "        \"task\": example[\"inputs\"][\"task\"],\n",
    "        \"response\": example[\"outputs\"][\"response\"],\n",
    "        \"task_id\": example[\"metadata\"][\"task_id\"],\n",
    "        \"MBPP_task_id\": example[\"metadata\"][\"MBPP_task_id\"],\n",
    "        \"external_functions\": example[\"metadata\"][\"external_functions\"]\n",
    "    }\n",
    "\n",
    "def dict_to_example(dict) -> Example:\n",
    "    return Example(\n",
    "        inputs=dict[\"task\"],\n",
    "        outputs=dict[\"response\"],\n",
    "        metadata=dict[\"MBPP_task_id\"]\n",
    "    )\n",
    "\n",
    "results = {}\n",
    "for model_name in models:\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    model = get_model_code_tokens_from_file(model_name, 'code_max_tokens.json')\n",
    "\n",
    "    validation_result = None\n",
    "    validation_result = Run(\n",
    "        phase=\"validation\",\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        top_k=10,\n",
    "        metric_results={\n",
    "            \"pass@1\": 0.2,\n",
    "        },\n",
    "        seed=None\n",
    "    )\n",
    "\n",
    "    if validation_result == None:\n",
    "  \n",
    "        validation_result = run_validation(\n",
    "            client,\n",
    "            model,\n",
    "            available_nodes,\n",
    "            val_data,\n",
    "            selector,\n",
    "            temperatures=[0.2, 0.6, 0.9],\n",
    "            top_ps=[0.2, 0.6, 0.9],\n",
    "            top_ks=[10, 50, 100],\n",
    "            ks=[1],\n",
    "            seed=9,\n",
    "            debug=True,\n",
    "            optimizer_metric=\"semantic\", # \"semantic\"  \"syntax\", \"tests\"\n",
    "        )\n",
    "    validation_result.print()\n",
    "\n",
    "    test_results = run_testing(\n",
    "        client,\n",
    "        model,\n",
    "        available_nodes,\n",
    "        test_data[:3],\n",
    "        selector,\n",
    "        temperature=validation_result.temperature,\n",
    "        top_p=validation_result.top_p,\n",
    "        top_k=validation_result.top_k,\n",
    "        ks=[1, 5], # k generations per task\n",
    "        seeds=[3, 75, 346],\n",
    "        debug=True,\n",
    "        metrics=[\"tests\"] # current metrics: \"semantic\",  \"syntax\", \"tests\"\n",
    "    )\n",
    "    results[model[\"name\"]] = (validation_result, test_results)\n",
    "      \n",
    "# Print the final results\n",
    "print(\"\\nFINAL RESULTS:\")\n",
    "for model_name, (val_run, test_runs) in results.items():\n",
    "    print(f\"{Style.BRIGHT}{Fore.CYAN} Model: {model_name} {Style.RESET_ALL}\")\n",
    "    val_run.print()\n",
    "    for test_run in test_runs:\n",
    "        test_run.print()\n",
    "    \n",
    "    result_run = calculate_final_result(test_runs)\n",
    "    result_run.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4307f",
   "metadata": {},
   "source": [
    "<!-- ## Langsmith evaluate -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
