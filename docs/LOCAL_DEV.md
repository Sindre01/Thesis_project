## ğŸŒ LocalÂ Development withÂ OllamaÂ API onÂ FOX

The steps below use bash commands to set up and run Ollama on the FOX cluster, allowing you to interact with it from your local machine.
The setup assumes you have SSH access to the FOX cluster and Ollama installed on it.
Steps 1-2 is for initial setup, and steps 3-5 (and more) is automated in script `run_interactive_ollama.sh`.

### 1Â Â·Â SSHÂ config

```bash
nano ~/.ssh/config
```

Add:

```text
Host fox
    HostName fox.educloud.no
    User <username>   # e.g., ec-sindrre
    IdentityFile ~/.ssh/id_rsa
    ControlMaster auto
    ControlPath ~/.ssh/sockets/%r@%h-%p
    ControlPersist 8h
    ServerAliveInterval 120
    ServerAliveCountMax 10
```

### 2Â Â·Â Start (or reuse) an SSHÂ master session

```bash
ssh fox   # asks for pwd & 2FA once per ControlPersist window
exit      # leave but keep master alive
```

Check/close:

```bash
ssh -O check fox   # status
ssh -O exit  fox   # stop master + sockets
```

### 3Â Â·Â Run **Ollama** on a compute/GPU node

```bash
#Â on gpuâ€‘10, gpuâ€‘11, ...             (never on the loginÂ node!)
# First a gpu node must be requested though slurm (batch job script with the command bellow OR an interactive session)
ollama serve
```

Healthâ€‘check:

```bash
curl localhost:11434   #Â â†’ â€œOllama is runningâ€
```

### 4Â Â·Â Portâ€‘forward the API to yourÂ laptop

```bash
ssh -f -N -L 11434:<gpu-nodename>:11434 fox
curl http://localhost:11434   # same â€œOllama is runningâ€ message
```

Now every local tool (Python, VSÂ Code, etc.) can use `http://localhost:11434` for accesing the ollama api running on the FOX cluster.

### 5Â Â·Â WrapÂ up

When done:

```bash
ssh -O exit fox   # closes port forward & master session
```

---

### OptionalÂ â€“ Custom model cache location

Home directory quota is tight; cache models under project storage instead:

```bash
export OLLAMA_MODELS=/cluster/work/projects/ec12/<username>/ollama-models
```

Add the line to your `~/.bashrc` on FOX and reâ€‘login.

---

## Steps 3-5 (and more) is automated in script `run_interactive_ollama.sh`